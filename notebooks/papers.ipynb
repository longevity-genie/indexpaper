{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Indexing dataset\n",
    "================"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extending pathes with local indexpaper\n",
      "['..', '/home/antonkulaga/sources/indexpaper/notebooks', '/home/antonkulaga/sources/indexpaper', '/home/antonkulaga/micromamba/envs/indexpaper/lib/python310.zip', '/home/antonkulaga/micromamba/envs/indexpaper/lib/python3.10', '/home/antonkulaga/micromamba/envs/indexpaper/lib/python3.10/lib-dynload', '', '/home/antonkulaga/.local/lib/python3.10/site-packages', '/home/antonkulaga/micromamba/envs/indexpaper/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "debug_local = True#False\n",
    "local = (Path(\"..\") / \"indexpaper\").resolve()\n",
    "if debug_local and local.exists():\n",
    "  sys.path.insert(0, Path(\"..\").as_posix())\n",
    "  print(\"extending pathes with local indexpaper\")\n",
    "  print(sys.path)\n",
    "  %load_ext autoreload\n",
    "  %autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T11:14:07.548365266Z",
     "start_time": "2023-10-03T11:14:07.547641647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T11:14:09.890748077Z",
     "start_time": "2023-10-03T11:14:08.295650938Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonkulaga/.local/lib/python3.10/site-packages/beartype/_util/hint/pep/utilpeptest.py:311: BeartypeDecorHintPep585DeprecationWarning: PEP 484 type hint typing.Callable[[list], list] deprecated by PEP 585. This hint is scheduled for removal in the first Python version released after October 5th, 2025. To resolve this, import this hint from \"beartype.typing\" rather than \"typing\". For further commentary and alternatives, see also:\n",
      "    https://beartype.readthedocs.io/en/latest/api_roar/#pep-585-deprecations\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "polars.config.Config"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from indexpaper import index\n",
    "\n",
    "from pycomfort.files import *\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from polars import Config\n",
    "Config.set_fmt_str_lengths(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/antonkulaga/.cache/torch/sentence_transformers/michiyasunaga_BioLinkBERT-large. Creating a new one with MEAN pooling.\n",
      "2023-10-03 14:14:15.884 | INFO     | indexpaper.indexing:init_qdrant:78 - initializing quadrant database at https://62d4a96e-2b91-4ab8-a4dd-a91e626d874a.europe-west3-0.gcp.cloud.qdrant.io:6333/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antonkulaga/sources/indexpaper/.env\n",
      "environment found at /home/antonkulaga/sources/indexpaper/.env\n"
     ]
    }
   ],
   "source": [
    "from pycomfort.config import load_environment_keys\n",
    "from indexpaper.paperset import Paperset\n",
    "from indexpaper.resolvers import *\n",
    "\n",
    "chunk_size = 512\n",
    "embeddings, splitter = resolve_embedding_splitter(EmbeddingType.HuggingFace, EmbeddingModels.biolinkbert.value, chunk_size=chunk_size)\n",
    "from indexpaper.indexing import init_qdrant\n",
    "load_environment_keys(usecwd=True)\n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "db = init_qdrant(\"biolinkbert_large_512_tacutu_papers\", \n",
    "                 path_or_url=\"https://62d4a96e-2b91-4ab8-a4dd-a91e626d874a.europe-west3-0.gcp.cloud.qdrant.io:6333/\", \n",
    "                 embeddings=embeddings, always_recreate=False,\n",
    "                 api_key=api_key\n",
    "                 )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T11:14:16.251749843Z",
     "start_time": "2023-10-03T11:14:09.891893324Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=0, indexed_vectors_count=0, points_count=0, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=True), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = db.client.get_collection(\"biolinkbert_large_512_tacutu_papers\")\n",
    "col"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T11:14:17.262985886Z",
     "start_time": "2023-10-03T11:14:17.197375970Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paperset #"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SearchRequest\nvector\n  none is not an allowed value (type=type_error.none.not_allowed)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Union\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#TODO: fix it does not work!\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mdb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbiolinkbert_large_512_tacutu_papers\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mFilter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmust\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mFieldCondition\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdoi\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMatchValue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m10.1186/s13059-020-01990-9\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/micromamba/envs/indexpaper/lib/python3.10/site-packages/qdrant_client/qdrant_client.py:315\u001B[0m, in \u001B[0;36mQdrantClient.search\u001B[0;34m(self, collection_name, query_vector, query_filter, search_params, limit, offset, with_payload, with_vectors, score_threshold, append_payload, consistency, **kwargs)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Search for closest vectors in collection taking into account filtering conditions\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \n\u001B[1;32m    251\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;124;03m    List of found close points with similarity scores.\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown arguments: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(kwargs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 315\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_vector\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_vector\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_filter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_filter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m    \u001B[49m\u001B[43msearch_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msearch_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[43m    \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwith_payload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_payload\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwith_vectors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_vectors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscore_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscore_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m    \u001B[49m\u001B[43mappend_payload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mappend_payload\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconsistency\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconsistency\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/micromamba/envs/indexpaper/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:456\u001B[0m, in \u001B[0;36mQdrantRemote.search\u001B[0;34m(self, collection_name, query_vector, query_filter, search_params, limit, offset, with_payload, with_vectors, score_threshold, append_payload, consistency, **kwargs)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(with_payload, grpc\u001B[38;5;241m.\u001B[39mWithPayloadSelector):\n\u001B[1;32m    451\u001B[0m     with_payload \u001B[38;5;241m=\u001B[39m GrpcToRest\u001B[38;5;241m.\u001B[39mconvert_with_payload_selector(with_payload)\n\u001B[1;32m    453\u001B[0m search_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhttp\u001B[38;5;241m.\u001B[39mpoints_api\u001B[38;5;241m.\u001B[39msearch_points(\n\u001B[1;32m    454\u001B[0m     collection_name\u001B[38;5;241m=\u001B[39mcollection_name,\n\u001B[1;32m    455\u001B[0m     consistency\u001B[38;5;241m=\u001B[39mconsistency,\n\u001B[0;32m--> 456\u001B[0m     search_request\u001B[38;5;241m=\u001B[39m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSearchRequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvector\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_vector\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_filter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    461\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msearch_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwith_vector\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_vectors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwith_payload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_payload\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscore_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    465\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    466\u001B[0m )\n\u001B[1;32m    467\u001B[0m result: Optional[List[types\u001B[38;5;241m.\u001B[39mScoredPoint]] \u001B[38;5;241m=\u001B[39m search_result\u001B[38;5;241m.\u001B[39mresult\n\u001B[1;32m    468\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSearch returned None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pydantic/main.py:341\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for SearchRequest\nvector\n  none is not an allowed value (type=type_error.none.not_allowed)"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http.models import *\n",
    "from typing import Union\n",
    "#TODO: fix it does not work!\n",
    "db.client.search(\"biolinkbert_large_512_tacutu_papers\", None,\n",
    "    Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key='doi',\n",
    "            match=MatchValue(value = \"10.1186/s13059-020-01990-9\")\n",
    "        )\n",
    "    ]\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T11:14:25.524936599Z",
     "start_time": "2023-10-03T11:14:24.907173625Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "shape: (1, 9)\n┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n│ corpusid  ┆ content_s ┆ updated   ┆ externali ┆ … ┆ annotatio ┆ annotatio ┆ annotatio ┆ annotati │\n│ ---       ┆ ource_oai ┆ ---       ┆ ds_doi    ┆   ┆ ns_abstra ┆ ns_author ┆ ns_title  ┆ ons_para │\n│ i64       ┆ nfo_opena ┆ str       ┆ ---       ┆   ┆ ct        ┆ ---       ┆ ---       ┆ graph    │\n│           ┆ ccessurl  ┆           ┆ str       ┆   ┆ ---       ┆ list[str] ┆ list[str] ┆ ---      │\n│           ┆ ---       ┆           ┆           ┆   ┆ list[str] ┆           ┆           ┆ list[str │\n│           ┆ str       ┆           ┆           ┆   ┆           ┆           ┆           ┆ ]        │\n╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n│ 259168060 ┆ null      ┆ 2023-07-0 ┆ 10.3389/f ┆ … ┆ [\"Song W  ┆ [\"Alexey  ┆ [\"OPEN    ┆ [\"It is  │\n│           ┆           ┆ 4T07:35:3 ┆ nagi.2023 ┆   ┆ (2023)    ┆ Moskalev  ┆ ACCESS    ┆ traditio │\n│           ┆           ┆ 5Z        ┆ .1179988  ┆   ┆ The glymp ┆ \",        ┆ EDITED BY ┆ nally    │\n│           ┆           ┆           ┆           ┆   ┆ hatic     ┆ \"Claudio  ┆ The glymp ┆ believed │\n│           ┆           ┆           ┆           ┆   ┆ system: a ┆ Nicoletti ┆ hatic     ┆ that the │\n│           ┆           ┆           ┆           ┆   ┆ new persp ┆ \", … \"    ┆ system: a ┆ lymphati │\n│           ┆           ┆           ┆           ┆   ┆ ective on ┆ Universit ┆ new persp ┆ c system │\n│           ┆           ┆           ┆           ┆   ┆ brain dis ┆ y of Pitt ┆ ective on ┆ doesn't  │\n│           ┆           ┆           ┆           ┆   ┆ eases.The ┆ sburgh    ┆ brain dis ┆ exist in │\n│           ┆           ┆           ┆           ┆   ┆ glymphati ┆ United    ┆ eases\",   ┆ the      │\n│           ┆           ┆           ┆           ┆   ┆ c system  ┆ States    ┆ \"OPEN     ┆ central  │\n│           ┆           ┆           ┆           ┆   ┆ is a brai ┆ \"]        ┆ ACCESS    ┆ nervous  │\n│           ┆           ┆           ┆           ┆   ┆ n-wide    ┆           ┆ EDITED BY ┆ system   │\n│           ┆           ┆           ┆           ┆   ┆ perivascu ┆           ┆ The glymp ┆ (Nederga │\n│           ┆           ┆           ┆           ┆   ┆ lar       ┆           ┆ hatic     ┆ ard and  │\n│           ┆           ┆           ┆           ┆   ┆ pathway   ┆           ┆ system: a ┆ Goldman, │\n│           ┆           ┆           ┆           ┆   ┆ driven by ┆           ┆ new persp ┆ 2016).   │\n│           ┆           ┆           ┆           ┆   ┆ aquaporin ┆           ┆ ective on ┆ Thus,    │\n│           ┆           ┆           ┆           ┆   ┆ -4 on the ┆           ┆ brain dis ┆ cell     │\n│           ┆           ┆           ┆           ┆   ┆ endfeet   ┆           ┆ eases\"]   ┆ debris,  │\n│           ┆           ┆           ┆           ┆   ┆ of astroc ┆           ┆           ┆ potentia │\n│           ┆           ┆           ┆           ┆   ┆ ytes,     ┆           ┆           ┆ l neurot │\n│           ┆           ┆           ┆           ┆   ┆ which can ┆           ┆           ┆ oxic pro │\n│           ┆           ┆           ┆           ┆   ┆ deliver   ┆           ┆           ┆ teins,   │\n│           ┆           ┆           ┆           ┆   ┆ nutrients ┆           ┆           ┆ and      │\n│           ┆           ┆           ┆           ┆   ┆ and       ┆           ┆           ┆ other    │\n│           ┆           ┆           ┆           ┆   ┆ active    ┆           ┆           ┆ metaboli │\n│           ┆           ┆           ┆           ┆   ┆ substance ┆           ┆           ┆ tes with │\n│           ┆           ┆           ┆           ┆   ┆ s to the  ┆           ┆           ┆ large    │\n│           ┆           ┆           ┆           ┆   ┆ brain par ┆           ┆           ┆ molecula │\n│           ┆           ┆           ┆           ┆   ┆ enchyma   ┆           ┆           ┆ r weight │\n│           ┆           ┆           ┆           ┆   ┆ through   ┆           ┆           ┆ are cons │\n│           ┆           ┆           ┆           ┆   ┆ periarter ┆           ┆           ┆ idered   │\n│           ┆           ┆           ┆           ┆   ┆ ial cereb ┆           ┆           ┆ to be    │\n│           ┆           ┆           ┆           ┆   ┆ rospinal  ┆           ┆           ┆ removed  │\n│           ┆           ┆           ┆           ┆   ┆ fluid     ┆           ┆           ┆ in a dif │\n│           ┆           ┆           ┆           ┆   ┆ (CSF)     ┆           ┆           ┆ ferent   │\n│           ┆           ┆           ┆           ┆   ┆ influx    ┆           ┆           ┆ clearanc │\n│           ┆           ┆           ┆           ┆   ┆ pathway   ┆           ┆           ┆ e        │\n│           ┆           ┆           ┆           ┆   ┆ and       ┆           ┆           ┆ pathway  │\n│           ┆           ┆           ┆           ┆   ┆ remove    ┆           ┆           ┆ than     │\n│           ┆           ┆           ┆           ┆   ┆ metabolic ┆           ┆           ┆ brain    │\n│           ┆           ┆           ┆           ┆   ┆ wastes    ┆           ┆           ┆ vasculat │\n│           ┆           ┆           ┆           ┆   ┆ through   ┆           ┆           ┆ ure. In  │\n│           ┆           ┆           ┆           ┆   ┆ perivenou ┆           ┆           ┆ 2012,    │\n│           ┆           ┆           ┆           ┆   ┆ s         ┆           ┆           ┆ the Nede │\n│           ┆           ┆           ┆           ┆   ┆ clearance ┆           ┆           ┆ rgaard   │\n│           ┆           ┆           ┆           ┆   ┆ routes.   ┆           ┆           ┆ (Iliff   │\n│           ┆           ┆           ┆           ┆   ┆ This      ┆           ┆           ┆ et al.,  │\n│           ┆           ┆           ┆           ┆   ┆ paper sum ┆           ┆           ┆ 2012)    │\n│           ┆           ┆           ┆           ┆   ┆ marizes   ┆           ┆           ┆ group    │\n│           ┆           ┆           ┆           ┆   ┆ the compo ┆           ┆           ┆ found    │\n│           ┆           ┆           ┆           ┆   ┆ sition,   ┆           ┆           ┆ that cer │\n│           ┆           ┆           ┆           ┆   ┆ overall   ┆           ┆           ┆ ebrospin │\n│           ┆           ┆           ┆           ┆   ┆ fluid     ┆           ┆           ┆ al fluid │\n│           ┆           ┆           ┆           ┆   ┆ flow,     ┆           ┆           ┆ (CSF)    │\n│           ┆           ┆           ┆           ┆   ┆ solute    ┆           ┆           ┆ can      │\n│           ┆           ┆           ┆           ┆   ┆ transport ┆           ┆           ┆ enter    │\n│           ┆           ┆           ┆           ┆   ┆ , related ┆           ┆           ┆ brain    │\n│           ┆           ┆           ┆           ┆   ┆ diseases, ┆           ┆           ┆ parenchy │\n│           ┆           ┆           ┆           ┆   ┆ affecting ┆           ┆           ┆ ma and   │\n│           ┆           ┆           ┆           ┆   ┆ factors,  ┆           ┆           ┆ exchange │\n│           ┆           ┆           ┆           ┆   ┆ and precl ┆           ┆           ┆ with     │\n│           ┆           ┆           ┆           ┆   ┆ inical    ┆           ┆           ┆ brain    │\n│           ┆           ┆           ┆           ┆   ┆ research  ┆           ┆           ┆ intersti │\n│           ┆           ┆           ┆           ┆   ┆ methods   ┆           ┆           ┆ tial     │\n│           ┆           ┆           ┆           ┆   ┆ of the    ┆           ┆           ┆ fluid    │\n│           ┆           ┆           ┆           ┆   ┆ glymphati ┆           ┆           ┆ (ISF) in │\n│           ┆           ┆           ┆           ┆   ┆ c system. ┆           ┆           ┆ the      │\n│           ┆           ┆           ┆           ┆   ┆ In doing  ┆           ┆           ┆ presence │\n│           ┆           ┆           ┆           ┆   ┆ so, we    ┆           ┆           ┆ of aquap │\n│           ┆           ┆           ┆           ┆   ┆ aim to    ┆           ┆           ┆ orin-4   │\n│           ┆           ┆           ┆           ┆   ┆ provide   ┆           ┆           ┆ (AQP4)   │\n│           ┆           ┆           ┆           ┆   ┆ direction ┆           ┆           ┆ on astro │\n│           ┆           ┆           ┆           ┆   ┆ and       ┆           ┆           ┆ cytes.   │\n│           ┆           ┆           ┆           ┆   ┆ reference ┆           ┆           ┆ Likewise │\n│           ┆           ┆           ┆           ┆   ┆ for more  ┆           ┆           ┆ , when   │\n│           ┆           ┆           ┆           ┆   ┆ relevant  ┆           ┆           ┆ the      │\n│           ┆           ┆           ┆           ┆   ┆ researche ┆           ┆           ┆ mixed    │\n│           ┆           ┆           ┆           ┆   ┆ rs in the ┆           ┆           ┆ fluid    │\n│           ┆           ┆           ┆           ┆   ┆ future.\"] ┆           ┆           ┆ was      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ drained  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ from the │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ brain,   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Amyloid- │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ β (Aβ)   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ was tran │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ sported  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ along    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ with the │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ outflow. │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Since    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ the      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ function │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ of this  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ \"drainag │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ e\"       │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ pathway  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ is       │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ similar  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ to that  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ of the   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ lymphati │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ c system │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ and supp │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ orted by │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ astrocyt │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ es, it   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ was      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ named    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ the glym │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ phatic   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ system   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ sooner   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ after    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ glia     │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ (Jessen  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ et al.,  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 2015).\", │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ \"Since   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ last     │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ decade,  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ many res │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ earchers │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ in the   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ field of │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ neurolog │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ y, neuro │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ degenera │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ tive     │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ diseases │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ and phys │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ iology   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ have     │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ aimed to │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ study    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ and      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ develop  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ the glym │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ phatic   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ system,  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ providin │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ g a      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ brandnew │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ perspect │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ive for  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ us to    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ understa │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ nd brain │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ diseases │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ : the    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ overall  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ fluid    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ flow of  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ the      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ brain    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ rather   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ than a   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ specific │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ lesion   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ or struc │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ture.    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Herein,  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ we summa │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ rized    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ the comp │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ onents   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ of the   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ glymphat │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ic       │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ system,  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ the      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ fluid    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ circulat │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ion mode │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ within   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ this     │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ system,  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ how path │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ogenic   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ solutes  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ are tran │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ sported  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ in       │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ certain  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ diseases │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ , affect │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ing      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ factors  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ of its   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ function │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ , and by │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ what     │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ means we │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ can      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ study    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ the glym │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ phatic   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ system.  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ All      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ above    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ may      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ provide  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ directio │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ n and    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ referenc │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ e for    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ brain    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ diseases │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ and      │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ medical  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ research │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ers.\", … │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ \"This    │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ work was │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ supporte │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ d by the │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Funds of │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ the scie │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ntific   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ and tech │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ nologica │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ l innova │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ tion     │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ project  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ of the   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Chinese  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Academy  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ of Tradi │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ tional   │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Chinese  │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Medicine │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ CI2021A0 │\n│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 4614.\"]  │\n└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘",
      "text/html": "<div><style>\n.dataframe > thead > tr > th,\n.dataframe > tbody > tr > td {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (1, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>corpusid</th><th>content_source_oainfo_openaccessurl</th><th>updated</th><th>externalids_doi</th><th>externalids_pubmed</th><th>annotations_abstract</th><th>annotations_author</th><th>annotations_title</th><th>annotations_paragraph</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>259168060</td><td>null</td><td>&quot;2023-07-04T07:35:35Z&quot;</td><td>&quot;10.3389/fnagi.2023.1179988&quot;</td><td>&quot;37396658&quot;</td><td>[&quot;Song W (2023) The glymphatic system: a new perspective on brain diseases.The glymphatic system is a brain-wide perivascular pathway driven by aquaporin-4 on the endfeet of astrocytes, which can deliver nutrients and active substances to the brain parenchyma through periarterial cerebrospinal fluid (CSF) influx pathway and remove metabolic wastes through perivenous clearance routes. This paper summarizes the composition, overall fluid flow, solute transport, related diseases, affecting factors, and preclinical research methods of the glymphatic system. In doing so, we aim to provide direction and reference for more relevant researchers in the future.&quot;]</td><td>[&quot;Alexey Moskalev &quot;, &quot;Claudio Nicoletti &quot;, … &quot;\nUniversity of Pittsburgh\nUnited States\n&quot;]</td><td>[&quot;OPEN ACCESS EDITED BY The glymphatic system: a new perspective on brain diseases&quot;, &quot;OPEN ACCESS EDITED BY The glymphatic system: a new perspective on brain diseases&quot;]</td><td>[&quot;It is traditionally believed that the lymphatic system doesn&#x27;t exist in the central nervous system (Nedergaard and Goldman, 2016). Thus, cell debris, potential neurotoxic proteins, and other metabolites with large molecular weight are considered to be removed in a different clearance pathway than brain vasculature. In 2012, the Nedergaard (Iliff et al., 2012) group found that cerebrospinal fluid (CSF) can enter brain parenchyma and exchange with brain interstitial fluid (ISF) in the presence of aquaporin-4 (AQP4) on astrocytes. Likewise, when the mixed fluid was drained from the brain, Amyloid-β (Aβ) was transported along with the outflow. Since the function of this &quot;drainage&quot; pathway is similar to that of the lymphatic system and supported by astrocytes, it was named the glymphatic system sooner after glia (Jessen et al., 2015).&quot;, &quot;Since last decade, many researchers in the field of neurology, neurodegenerative diseases and physiology have aimed to study and develop the glymphatic system, providing a brandnew perspective for us to understand brain diseases: the overall fluid flow of the brain rather than a specific lesion or structure. Herein, we summarized the components of the glymphatic system, the fluid circulation mode within this system, how pathogenic solutes are transported in certain diseases, affecting factors of its function, and by what means we can study the glymphatic system. All above may provide direction and reference for brain diseases and medical researchers.&quot;, … &quot;This work was supported by the Funds of the scientific and technological innovation project of the Chinese Academy of Traditional Chinese Medicine CI2021A04614.&quot;]</td></tr></tbody></table></div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moskalev = Paperset(\"longevity-genie/moskalev_papers\", splitter=splitter)\n",
    "moskalev.lazy_frame.collect().head(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T11:14:42.064949608Z",
     "start_time": "2023-10-03T11:14:39.386811148Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "CollectionsResponse(collections=[CollectionDescription(name='biolinkbert_large_512_tacutu_papers')])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = db.client.get_collections()\n",
    "col"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T11:15:10.117184193Z",
     "start_time": "2023-10-03T11:15:09.780776076Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indexing\n",
    "========\n",
    "To avoid memory overflow we index everything by slices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmoskalev\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_by_slices\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<@beartype(indexpaper.paperset.Paperset.index_by_slices) at 0x7fea9be2d360>:67\u001B[0m, in \u001B[0;36mindex_by_slices\u001B[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_94807790033808, *args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/sources/indexpaper/notebooks/../indexpaper/paperset.py:222\u001B[0m, in \u001B[0;36mPaperset.index_by_slices\u001B[0;34m(self, n, db, start)\u001B[0m\n\u001B[1;32m    219\u001B[0m     ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_id_from_data(d\u001B[38;5;241m.\u001B[39mpage_content) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[1;32m    220\u001B[0m     db\u001B[38;5;241m.\u001B[39madd_texts(texts\u001B[38;5;241m=\u001B[39mtexts, metadatas\u001B[38;5;241m=\u001B[39mmetadatas, ids\u001B[38;5;241m=\u001B[39mids)\n\u001B[0;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforeach_document_slice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_paper_slice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/sources/indexpaper/notebooks/../indexpaper/paperset.py:192\u001B[0m, in \u001B[0;36mPaperset.foreach_document_slice\u001B[0;34m(self, n, fun, start)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfun_df\u001B[39m(df: pl\u001B[38;5;241m.\u001B[39mDataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocuments_from_dataset_slice(df))\n\u001B[0;32m--> 192\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforeach_slice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfun_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/sources/indexpaper/notebooks/../indexpaper/paperset.py:183\u001B[0m, in \u001B[0;36mPaperset.foreach_slice\u001B[0;34m(self, n, fun, start)\u001B[0m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;66;03m# Apply the function to the slice (in place modification)\u001B[39;00m\n\u001B[0;32m--> 183\u001B[0m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslice_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;66;03m# Recursive call to process the next slice\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforeach_slice(n, fun, start \u001B[38;5;241m+\u001B[39m n)\n",
      "File \u001B[0;32m~/sources/indexpaper/notebooks/../indexpaper/paperset.py:191\u001B[0m, in \u001B[0;36mPaperset.foreach_document_slice.<locals>.fun_df\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfun_df\u001B[39m(df: pl\u001B[38;5;241m.\u001B[39mDataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdocuments_from_dataset_slice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/sources/indexpaper/notebooks/../indexpaper/utils.py:21\u001B[0m, in \u001B[0;36mtiming.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     20\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m---> 21\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Call the original function\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     23\u001B[0m     execution_time \u001B[38;5;241m=\u001B[39m end_time \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m~/sources/indexpaper/notebooks/../indexpaper/paperset.py:220\u001B[0m, in \u001B[0;36mPaperset.index_by_slices.<locals>.index_paper_slice\u001B[0;34m(docs)\u001B[0m\n\u001B[1;32m    218\u001B[0m metadatas \u001B[38;5;241m=\u001B[39m [d\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[1;32m    219\u001B[0m ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_id_from_data(d\u001B[38;5;241m.\u001B[39mpage_content) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[0;32m--> 220\u001B[0m \u001B[43mdb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mids\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/micromamba/envs/indexpaper/lib/python3.10/site-packages/langchain/vectorstores/qdrant.py:178\u001B[0m, in \u001B[0;36mQdrant.add_texts\u001B[0;34m(self, texts, metadatas, ids, batch_size, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Run more texts through the embeddings and add to the vectorstore.\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \n\u001B[1;32m    164\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;124;03m    List of ids from adding the texts into the vectorstore.\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    177\u001B[0m added_ids \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 178\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_ids, points \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_rest_batches(\n\u001B[1;32m    179\u001B[0m     texts, metadatas, ids, batch_size\n\u001B[1;32m    180\u001B[0m ):\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mupsert(\n\u001B[1;32m    182\u001B[0m         collection_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollection_name, points\u001B[38;5;241m=\u001B[39mpoints, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    183\u001B[0m     )\n\u001B[1;32m    184\u001B[0m     added_ids\u001B[38;5;241m.\u001B[39mextend(batch_ids)\n",
      "File \u001B[0;32m~/micromamba/envs/indexpaper/lib/python3.10/site-packages/langchain/vectorstores/qdrant.py:2072\u001B[0m, in \u001B[0;36mQdrant._generate_rest_batches\u001B[0;34m(self, texts, metadatas, ids, batch_size)\u001B[0m\n\u001B[1;32m   2069\u001B[0m batch_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(islice(ids_iterator, batch_size))\n\u001B[1;32m   2071\u001B[0m \u001B[38;5;66;03m# Generate the embeddings for all the texts in a batch\u001B[39;00m\n\u001B[0;32m-> 2072\u001B[0m batch_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_embed_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_texts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2074\u001B[0m points \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   2075\u001B[0m     rest\u001B[38;5;241m.\u001B[39mPointStruct(\n\u001B[1;32m   2076\u001B[0m         \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39mpoint_id,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2091\u001B[0m     )\n\u001B[1;32m   2092\u001B[0m ]\n\u001B[1;32m   2094\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m batch_ids, points\n",
      "File \u001B[0;32m~/micromamba/envs/indexpaper/lib/python3.10/site-packages/langchain/vectorstores/qdrant.py:2012\u001B[0m, in \u001B[0;36mQdrant._embed_texts\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Embed search texts.\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \n\u001B[1;32m   2003\u001B[0m \u001B[38;5;124;03mUsed to provide backward compatibility with `embedding_function` argument.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2009\u001B[0m \u001B[38;5;124;03m    List of floats representing the texts embedding.\u001B[39;00m\n\u001B[1;32m   2010\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2011\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2012\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2013\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(embeddings, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtolist\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   2014\u001B[0m         embeddings \u001B[38;5;241m=\u001B[39m embeddings\u001B[38;5;241m.\u001B[39mtolist()\n",
      "File \u001B[0;32m~/micromamba/envs/indexpaper/lib/python3.10/site-packages/langchain/embeddings/huggingface.py:92\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.embed_documents\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m     90\u001B[0m     sentence_transformers\u001B[38;5;241m.\u001B[39mSentenceTransformer\u001B[38;5;241m.\u001B[39mstop_multi_process_pool(pool)\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 92\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m embeddings\u001B[38;5;241m.\u001B[39mtolist()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001B[0m\n\u001B[1;32m    162\u001B[0m features \u001B[38;5;241m=\u001B[39m batch_to_device(features, device)\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 165\u001B[0m     out_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m output_value \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_embeddings\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    168\u001B[0m         embeddings \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, features)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m features:\n\u001B[1;32m     64\u001B[0m     trans_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 66\u001B[0m output_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mauto_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtrans_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m output_tokens \u001B[38;5;241m=\u001B[39m output_states[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     69\u001B[0m features\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_embeddings\u001B[39m\u001B[38;5;124m'\u001B[39m: output_tokens, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m: features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]})\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1020\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1011\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m   1013\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m   1014\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1015\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1018\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m   1019\u001B[0m )\n\u001B[0;32m-> 1020\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1028\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1030\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1032\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1033\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:610\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    601\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    602\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    603\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    607\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    608\u001B[0m     )\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 610\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    620\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:495\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    484\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    485\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    492\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    493\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    494\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 495\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:425\u001B[0m, in \u001B[0;36mBertAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    417\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    423\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    424\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 425\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    435\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:307\u001B[0m, in \u001B[0;36mBertSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    306\u001B[0m     key_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey(hidden_states))\n\u001B[0;32m--> 307\u001B[0m     value_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    309\u001B[0m query_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(mixed_query_layer)\n\u001B[1;32m    311\u001B[0m use_cache \u001B[38;5;241m=\u001B[39m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "moskalev.index_by_slices(10, db, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T11:17:01.567338262Z",
     "start_time": "2023-10-03T11:15:19.639868526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Testing index\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T12:39:30.888141717Z",
     "start_time": "2023-08-06T12:39:30.708993348Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db.similarity_search(\"IKK-β activation \")[0].page_content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T11:17:01.612670165Z",
     "start_time": "2023-10-03T11:17:01.611646090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'proteins: p65 (RelA), RelB, c-Rel, p105/p50 (NF-κB1), and p100/52 (NF-κB2), which associate with each other to form distinct active NF-κB dimers. In the cytosol, NF-κB dimers in inactive form anchored by IκB are found. There are two ways to activate the NF-kB-induced gene transcription. These are triggered by cytokines TNFα and IL-1 (canonical signaling) or antigen receptors CD40 and BAFF (non-canonical/alternative signaling). There are the activation of IKK complex (IKKα, IKKβ, and IKKγ) and the phosphorylation of IκB proteins, which are in interaction with'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"IKK-β activation \")[0].page_content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T12:41:43.302063403Z",
     "start_time": "2023-08-06T12:41:43.202131762Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
