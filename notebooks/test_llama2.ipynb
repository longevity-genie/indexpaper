{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:04:34.502884031Z",
     "start_time": "2023-07-20T15:04:34.475663772Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pycomfort.files import tprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\n",
      "\t.gitignore\n",
      "\tllama-2-13b-chat.ggmlv3.q2_K.bin\n",
      "\tllama-2-13b.ggmlv3.q2_K.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base = Path(\"..\").absolute().resolve()\n",
    "models = base / \"data\" / \"models\"\n",
    "tprint(models)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:04:35.024403131Z",
     "start_time": "2023-07-20T15:04:35.018949284Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up LLama 2 ##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:05:16.034292188Z",
     "start_time": "2023-07-20T15:05:15.990540100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:05:16.928129952Z",
     "start_time": "2023-07-20T15:05:16.918462510Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:05:52.185499986Z",
     "start_time": "2023-07-20T15:05:52.177661585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "PosixPath('/home/antonkulaga/sources/getpaper/data/models/llama-2-13b-chat.ggmlv3.q2_K.bin')"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = models / \"llama-2-13b-chat.ggmlv3.q2_K.bin\" #\"llama-2-13b.ggmlv3.q2_K.bin\"\n",
    "model_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:06:18.826905559Z",
     "start_time": "2023-07-20T15:06:18.821169947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pdfminer' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpdfminer\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mpdfminer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__version__\u001B[49m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'pdfminer' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import pdfminer\n",
    "print(pdfminer.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T22:33:02.607901187Z",
     "start_time": "2023-07-20T22:33:02.560612423Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/antonkulaga/sources/getpaper/data/models/llama-2-13b-chat.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 7089.00 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=str(model_path),\n",
    "    input={\"temperature\": 0.0, \"max_length\": 2000, \"top_p\": 1},\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:06:24.011666960Z",
     "start_time": "2023-07-20T15:06:23.717947975Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dear Sam Altman,\n",
      "\n",
      "I hope this message finds you well. As a long-time admirer of your work in the field of artificial intelligence, I am writing to express my deepest interest in the incredible technology that OpenAI has developed, specifically ChatGPT 4 and future models. As an aspiring AI researcher myself, I believe that open-sourcing these models would not only benefit humanity but also further propel the field of AI forward.\n",
      "\n",
      "In my opinion, there are several compelling reasons why OpenAI should consider open-sourcing ChatGPT 4 and future models. Firstly, open-sourcing would allow researchers and developers from around the world to collaborate and build upon your technology, potentially leading to breakthroughs in various fields such as natural language processing or machine learning. Secondly, open-sourcing would provide an opportunity for a broader range of individuals and organizations to experiment with and learn about AI, which could help demystify the technology and promote greater adoption and integration into society. Lastly, by open-sourcing your models, OpenAI can demonstrate its commitment to the greater good and transparency"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2219.53 ms\n",
      "llama_print_timings:      sample time =   251.65 ms /   256 runs   (    0.98 ms per token,  1017.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8779.31 ms /    31 tokens (  283.20 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:        eval time = 105212.17 ms /   255 runs   (  412.60 ms per token,     2.42 tokens per second)\n",
      "llama_print_timings:       total time = 115344.91 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nDear Sam Altman,\\n\\nI hope this message finds you well. As a long-time admirer of your work in the field of artificial intelligence, I am writing to express my deepest interest in the incredible technology that OpenAI has developed, specifically ChatGPT 4 and future models. As an aspiring AI researcher myself, I believe that open-sourcing these models would not only benefit humanity but also further propel the field of AI forward.\\n\\nIn my opinion, there are several compelling reasons why OpenAI should consider open-sourcing ChatGPT 4 and future models. Firstly, open-sourcing would allow researchers and developers from around the world to collaborate and build upon your technology, potentially leading to breakthroughs in various fields such as natural language processing or machine learning. Secondly, open-sourcing would provide an opportunity for a broader range of individuals and organizations to experiment with and learn about AI, which could help demystify the technology and promote greater adoption and integration into society. Lastly, by open-sourcing your models, OpenAI can demonstrate its commitment to the greater good and transparency'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Write a letter to Sam Altman, CEO of OpenAI, persuading him to open-source ChatGPT 4 and future models\n",
    "\"\"\"\n",
    "llm(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:10:24.575945781Z",
     "start_time": "2023-07-20T15:08:29.216414301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
