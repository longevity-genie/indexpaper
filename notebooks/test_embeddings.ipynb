{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-31T00:44:28.758692077Z",
     "start_time": "2023-07-31T00:39:30.389988522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)8d1f6/.gitattributes:   0%|          | 0.00/1.63k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3e365e3db0e44439823a1b38182e2e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)_Pooling/config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99b6e5edaae2420d96c1fb32c7629361"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)5ae7c8d1f6/README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb002b21906e4935b6ea84a275ad5d71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)e7c8d1f6/config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ed770b70e7448d28aa64675f796d0e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b26adeb50244c2799ca711a251bab36"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)1f6/onnx/config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ba63a9c908e44cbbe9e2fde4e6c87b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading model.onnx:   0%|          | 0.00/546k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a24266938514d9188710e5bab518544"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading model.onnx_data:   0%|          | 0.00/2.24G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6c2743a643140b08814a0bfabfe3688"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e1dbf88a6eb4b12b638419ff7fcf467"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bdab62429f7496888a898e15cd817ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "879eb982b78949368765adf531380fa6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3a26d198c524877b2517bd2f8aa6615"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78eba86a34724c47928fbd68dfc07c1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)nce_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8a6cbdc7dd644eeab3434b0c4af37ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2e3e3d648504dae90ea49cb31f4033e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc397d322233478ea0aee9c71dfa2a2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dcfe44ee29d4faa8b757bedab5b83c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "874000fc6adf40d7b64c34735046b094"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)7c8d1f6/modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cf4dadf7c674b4abcd896562796171d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[Document(page_content='This is a document about the Boston Celtics', metadata={}),\n Document(page_content='The Celtics are my favourite team.', metadata={}),\n Document(page_content='The Boston Celtics won the game by 20 points', metadata={}),\n Document(page_content='L. Kornet is one of the best Celtics players.', metadata={}),\n Document(page_content='Basquetball is a great sport.', metadata={}),\n Document(page_content='Elden Ring is one of the best games in the last 15 years.', metadata={}),\n Document(page_content='This is just a random text.', metadata={}),\n Document(page_content='Fly me to the moon is one of my favourite songs.', metadata={}),\n Document(page_content='I simply love going to the movies', metadata={}),\n Document(page_content='Larry Bird was an iconic NBA player.', metadata={})]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, LlamaCppEmbeddings, OpenAIEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large\" #\"thenlper/gte-large\" #\"menadsa/S-BioELECTRA\"\n",
    "# Get embeddings.\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "#embeddings = LlamaCppEmbeddings(model_path = \"/home/antonkulaga/sources/longevity-genie/data/models/stablebeluga-13b.ggmlv3.q2_K.bin\")\n",
    "texts = [\n",
    "    \"Basquetball is a great sport.\",\n",
    "    \"Fly me to the moon is one of my favourite songs.\",\n",
    "    \"The Celtics are my favourite team.\",\n",
    "    \"This is a document about the Boston Celtics\",\n",
    "    \"I simply love going to the movies\",\n",
    "    \"The Boston Celtics won the game by 20 points\",\n",
    "    \"This is just a random text.\",\n",
    "    \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "    \"L. Kornet is one of the best Celtics players.\",\n",
    "    \"Larry Bird was an iconic NBA player.\",\n",
    "]\n",
    "# Create a retriever\n",
    "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "14"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sentence with 10 tokens. Can we find more?\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large\")\n",
    "len(tokenizer.tokenize(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-30T23:02:45.193966645Z",
     "start_time": "2023-07-30T23:02:44.844778594Z"
    }
   },
   "id": "92783cae9e28e2df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute speed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "212c7e9e77d35566"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9bcce9c15b2476f989b3c9eee7ed056"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/670M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6f8dac262c34e1fa9112fb4d5707eb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "input_texts = [\n",
    "    \"what is the capital of China?\",\n",
    "    \"how to implement quick sort in python?\",\n",
    "    \"Beijing\",\n",
    "    \"sorting algorithms\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large\")\n",
    "model = AutoModel.from_pretrained(\"thenlper/gte-large\")\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T09:07:33.721037711Z",
     "start_time": "2023-07-31T09:07:02.934333073Z"
    }
   },
   "id": "911c34aa13715fa4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compute_embeddings(input_texts: list[str]):\n",
    "    start_time = time.perf_counter()  # Start the timer\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "    end_time = time.perf_counter()  # End the timer\n",
    "\n",
    "    execution_time = end_time - start_time  # Calculate the execution time\n",
    "\n",
    "    return execution_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T09:09:24.570854446Z",
     "start_time": "2023-07-31T09:09:24.527034008Z"
    }
   },
   "id": "914ec206e9d0caf9"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0.2508821910014376"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_embeddings(input_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T09:09:33.746088219Z",
     "start_time": "2023-07-31T09:09:33.483360147Z"
    }
   },
   "id": "3e5ed85661c37c64"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "['This is a sentence with 10 tokens. Can we find more?']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large\")\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "texts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-30T23:07:10.587351382Z",
     "start_time": "2023-07-30T23:07:10.412990253Z"
    }
   },
   "id": "9beb589450393015"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'tokenize'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is a sentence with 10 tokens. Can we find more?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Tokenize the text\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m tokens \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Count the number of tokens\u001B[39;00m\n\u001B[1;32m     13\u001B[0m token_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokens)\n",
      "File \u001B[0;32m~/micromamba/envs/longevity-genie/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:319\u001B[0m, in \u001B[0;36mSentenceTransformer.tokenize\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, texts: Union[List[\u001B[38;5;28mstr\u001B[39m], List[Dict], List[Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]]]):\n\u001B[1;32m    316\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;124;03m    Tokenizes the texts\u001B[39;00m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 319\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_first_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m(texts)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'tokenize'"
     ]
    }
   ],
   "source": [
    "import sentence_transformers as st\n",
    "# Load the sentence transformer model\n",
    "model = st.SentenceTransformer()\n",
    "\n",
    "# Get the text\n",
    "text = \"This is a sentence with 10 tokens. Can we find more?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = model.tokenize(text)\n",
    "\n",
    "# Count the number of tokens\n",
    "token_count = len(tokens)\n",
    "\n",
    "# Print the number of tokens\n",
    "print(f\"The number of tokens in the text is {token_count}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-30T23:01:43.969393785Z",
     "start_time": "2023-07-30T23:01:43.820011073Z"
    }
   },
   "id": "152b142fdd10353d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5747.48 ms /     8 tokens (  718.44 ms per token,     1.39 tokens per second)\n",
      "llama_print_timings:        eval time =  1103.68 ms /     1 runs   ( 1103.68 ms per token,     0.91 tokens per second)\n",
      "llama_print_timings:       total time =  6868.26 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 10178.43 ms /    15 tokens (  678.56 ms per token,     1.47 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 10186.59 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  7520.81 ms /    11 tokens (  683.71 ms per token,     1.46 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  7530.58 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  8036.52 ms /    11 tokens (  730.59 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  8045.39 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  3741.44 ms /     8 tokens (  467.68 ms per token,     2.14 tokens per second)\n",
      "llama_print_timings:        eval time =   394.17 ms /     1 runs   (  394.17 ms per token,     2.54 tokens per second)\n",
      "llama_print_timings:       total time =  4143.67 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5415.06 ms /    14 tokens (  386.79 ms per token,     2.59 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5420.77 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  3925.39 ms /     8 tokens (  490.67 ms per token,     2.04 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  3929.13 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  8147.07 ms /    19 tokens (  428.79 ms per token,     2.33 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  8156.63 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  8914.08 ms /    15 tokens (  594.27 ms per token,     1.68 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  8921.79 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5707.58 ms /    11 tokens (  518.87 ms per token,     1.93 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5716.64 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  7174.67 ms /    12 tokens (  597.89 ms per token,     1.67 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  7180.08 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Document(page_content='The Celtics are my favourite team.', metadata={}),\n Document(page_content='L. Kornet is one of the best Celtics players.', metadata={}),\n Document(page_content='Basquetball is a great sport.', metadata={}),\n Document(page_content='Fly me to the moon is one of my favourite songs.', metadata={}),\n Document(page_content='Larry Bird was an iconic NBA player.', metadata={}),\n Document(page_content='This is just a random text.', metadata={}),\n Document(page_content='Elden Ring is one of the best games in the last 15 years.', metadata={}),\n Document(page_content='I simply love going to the movies', metadata={}),\n Document(page_content='This is a document about the Boston Celtics', metadata={}),\n Document(page_content='The Boston Celtics won the game by 20 points', metadata={})]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a retriever\n",
    "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-29T01:17:33.248633723Z",
     "start_time": "2023-07-29T01:16:16.933149224Z"
    }
   },
   "id": "375833538ba59778"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  2510.96 ms /     8 tokens (  313.87 ms per token,     3.19 tokens per second)\n",
      "llama_print_timings:        eval time =   338.84 ms /     1 runs   (  338.84 ms per token,     2.95 tokens per second)\n",
      "llama_print_timings:       total time =  2852.88 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5234.82 ms /    15 tokens (  348.99 ms per token,     2.87 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5237.32 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  3837.39 ms /    11 tokens (  348.85 ms per token,     2.87 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  3843.08 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4048.82 ms /    11 tokens (  368.07 ms per token,     2.72 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4055.46 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  3207.04 ms /     8 tokens (  400.88 ms per token,     2.49 tokens per second)\n",
      "llama_print_timings:        eval time =   490.39 ms /     1 runs   (  490.39 ms per token,     2.04 tokens per second)\n",
      "llama_print_timings:       total time =  3704.55 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  8445.20 ms /    14 tokens (  603.23 ms per token,     1.66 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  8454.35 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4967.16 ms /     8 tokens (  620.89 ms per token,     1.61 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4969.92 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 11347.70 ms /    19 tokens (  597.25 ms per token,     1.67 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 11363.78 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  8284.26 ms /    15 tokens (  552.28 ms per token,     1.81 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  8301.43 ms\n",
      "\n",
      "llama_print_timings:        load time =  5457.98 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5827.61 ms /    11 tokens (  529.78 ms per token,     1.89 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5832.48 ms\n"
     ]
    }
   ],
   "source": [
    "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-29T01:18:48.697253154Z",
     "start_time": "2023-07-29T01:17:49.906679768Z"
    }
   },
   "id": "7039083fcd076a33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a8fb4b06d1e73b82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
